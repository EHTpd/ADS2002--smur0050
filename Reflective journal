Week 1
The starting week, we started with introductions and an overview of the course, 
including group projects such as the catheter placement project, a stock tweet data analysis 
and solar activity activity project. Despite my initial nervousness about the upcoming workload,
seeing familiar faces in the classes made me more comfortable with interaction and group work. 
Out of the possible projects, I was drawn to the solar activity project, as it aligns with my interest in 
analysing natural phenomena using data. Moreover we were tasked in selecting an Australian company for an essay focused on the said business relationship with big data. I chose QantasAirlines as it intrigued by how they use data to enhance operational efficiency. This week taught me the importance of having a familiar surroundings and a clear focus on my interests when transitioning into a new environment. Moving forward I plan to actively engage with the solar activity project and leverage available resources to strengthen my knowledge in data analysis. 
For my essay, I will explore Qantas’s use of big on how data driven decision making is applied.


Week 2
This week marked the beginning of our work with a coding platform called Google Colab, 
which was a shift from the Jupyter Notebook environment I am used to.
Initially I found the transition challenging as I found myself being stuck as I tried to adapt to the different interface and functionality that
Colab offers. Despite these initial difficulties, I dedicated time to learn its features, 
experimenting with its tools and seeking help with my peers when needed.
This eventually allowed me to adapt to the new environment. On top of this,
I was excited to eb assignment to the solar activity projects,
working with a  team of six peers who share similar interests. We had a comprehensive overview of the project objectives and scope with our mentor. 
This talk was quite informative as we discussed various ways we can attack our project. 
Also looked into a way how we model the solar activity data, such as LSTM networks and Convolutional Neural Networks. 
The complexity of these models feel like a huge task, but I feel enthusiastic that I can gain valuable skill and knowledge from doing my tasks.

Week 3
This week in class on learning about gradient descent and its application to 
linear regression using the given diabetes dataset. 
We practised with the Vannial Gradient Descent to understand how this algorithm helps minimise errors 
by iteratively adjusting the models parameters. 
This hands on experience made it clear how gradient descent worked. 
For the group work for the solar activity projects, we stared by researching the variables in the dataset. 
The research showed how the Auroral Electrojet index is derived from data collected at aground mangemeter
observatories in the Northern Hemisphere and measures magnetic activity in the auroral zone.
It's calculated using the difference between the auroral electrojet upper and lower indices,
which reflects the intensity of the eastward and westward electrojets. 
Even though we now have a better understanding of these indices, 
I'm still kinda unclear about the exact goal of our project. I believe with more meetings and discussions,
we’ll be able to find a clear research question.


Week 4

This week the classwork focused on understanding the perceptron model,
building on the model with linear regression through covering topics like the perceptron model and 
perceptron cost function. With the group work on solar activity,
we imported and cleaned the dataset which include the variables as previously mentioned 
like AE, AU, and AL indices, not to mention the date, time day and Disturbance storm time index.
After cleaning the dataset included entries from jan 1 1957 to August 31 2024, 
we conducted some initial processing and analysis but creating time series plots for the dst values for a random year in the data. 
In this case it as 2004 to understand the data better, Similar analysis were done with the Distance to Extreme values.
To manage our project efficiently we also discussed how to allocate specific 
tasks to each group element to ensure that everyone is contribution. 
Thus to create a well coordinated team we aim to synchronise our effort and divide the group work. 


Week 5
This week, the classwork went through Multilayer Perceptrons(MLPs) backpropagation,
gradient descent, activation function and how to use TensorFlow and Kera.
We practised by processing and loading data, creating and training models, and evaluating their accuracy. 
This gave us hands-on experience with optimising and building neural networks. 
For our solar activity project, we performed a cluster analysis on the DST values, 
splitting them into three categories. Cluster 0 (purple) had DST values near zero,
indicating low geomagnetic activity. Cluster 1 (Teal) had moderately negative DST values, suggesting mild geomagnetic storms.
Cluster 2 (Yellow) had highly negative DST values, indicating strong geomagnetic storms and significant space weather events. 
We used the Elbow method to find the optimal number of clusters and calculate the silhouette score to assess the quality of the clustering.
These methods helped us better understand patterns in geomagnetic activity.
For the future we need to be more organised in terms of what work each individual does and to find an overall question that the group needs to answer to prepare
for the mock presentation.

Week 6
This week, the class focused on neural networks. 
We worked on the Auto MPG dataset from UCI Machine Learning Repository, 
exploring both linear and nonlinear regression models and analysing their performance. 
The exercises provided practical experience for us to implement and evaluate different types of regression models using neural networks. 
In preparation for our mock presentation, our group has finally decided on a goal: to focus on predicting recovery time and actual storms using DST values.
Each partnership within the group will be responsible for using different models to predict the storms and recovery time. These included CNN and LSTM models. 
We have scheduled a meeting for Tuesday 430 to review our presentation and make any necessary adjustments. 
The group started working on the introduction of our report, but I'm still feeling nervous about the presentation and need to make sure the group is well prepared.


Week 7

In Week 7, I focused on submitting my essay about Virgin Airlines and its role in utilizing big data to enhance operational efficiency and 
customer experience. This task allowed me to explore the intersection of aviation and data analytics, highlighting how Virgin Airlines leverages data 
for strategic decision-making. Additionally, I delved into neural network exercises using TensorFlow, concentrating on the concepts of overfitting and underfitting.
To reinforce my understanding, I applied these concepts to the fuel efficiency dataset, developing a regression model that illustrated the implications of model complexity 
on predictive performance. This week was a valuable opportunity to synthesize my coursework with practical applications in the context of big data and machine learning.

Week 8
In Week 8, I focused on image classification exercises and an introduction to Convolutional Neural Networks (CNNs). The notebook I 
worked through introduced essential concepts for understanding basic CNNs, which are vital for image processing. It covered definitions 
and workings of convolutions, explaining how they act as filters that learn local patterns through convolutional layers. 
Additionally, I engaged in group work reflecting on a mock presentation. The feedback highlighted several areas for improvement, such as 
enhancing the overall flow and consistency of the presentation. It noted the quality differences in slide structure across sections and 
recommended adopting more professional body language. The need to maintain better eye contact with the audience, listen actively to team members, and rehearse the delivery was emphasized. 

Week 9
This week, I focused on enhancing my skills in neural networks and image classification, as well as reflecting on my presentation abilities. 
I worked on exercises involving overfitting and underfitting using TensorFlow by implementing a regression model with the fuel efficiency dataset,
which provided practical insights into the challenges of training models effectively and balancing bias and variance. Additionally,
I made significant strides in my project on classifying CIFAR images with a Convolutional Neural Network (CNN) using Keras, delving into the model's 
architecture and exploring the implementation of various layers such as 2D convolutional and max-pooling layers. I also focused on the training process and 
conducted model evaluations using confusion matrices to assess the model's classification performance. Furthermore, I took time to reflect on a recent mock
presentation and found the feedback I received on its structure, delivery, and content invaluable, which I plan to apply to improve my presentation skills for future assignments.
My goals for next week include continuing to refine my CNN model for better performance, exploring techniques to mitigate overfitting, and preparing for upcoming presentations by integrating the feedback I received.

Week 10


This week, I focused on model interpretability in machine learning, particularly with linear models, Decision Trees, and modern techniques like LIME and SHAP.
I appreciated the simplicity of linear models, where the coefficients clearly show how features impact predictions. Transitioning to tree-based models, 
I explored feature importance metrics, which highlight key variables but lack individual prediction explanations. Implementing LIME and SHAP on the Abalone dataset was particularly enlightening.
LIME helped me understand specific predictions, while SHAP provided a consistent view of feature contributions across all predictions. In our group meetings,
we discussed our implementation of these interpretability methods. Collaborating with my peers allowed us to share insights and optimise our code effectively.
Overall, this week reinforced the importance of making machine learning models more transparent and the value of teamwork in tackling complex topics.

Week 11
I focused on enhancing my understanding of model interpretability in machine learning, particularly through LIME (Local Interpretable Model-agnostic Explanations)
and SHAP (Shapley Additive explanations). I revisited the Abalone dataset to explore feature importance and the implications for model transparency, 
noting the benefits of linear models for interpretability and the clear relationship between coefficients and predictions. Additionally, 
I collaborated with my group on our LSTM project, preparing for a crucial meeting to discuss our findings and finalize our model configurations. 
We organized our agenda, outlining key points to cover, including model performance metrics and feature selection strategies.
The week culminated in successfully installing LIME and SHAP libraries in my Colab environment, conducting experiments that illustrated
how LIME assesses the impact of individual feature changes on model predictions and how SHAP quantifies each feature's contribution to overall predictions. 
This hands-on experience provided me with valuable insights into the practical applications of these interpretability techniques, reinforcing the importance of understanding model behaviour for informed decision-making.

Week 12
In Week 12, our focus was entirely on preparing for the group presentation. We held two meetings to discuss the logistics, such as the structure of
the presentation, what content to include, and who would be responsible for each section. My role was to introduce the context, specifically explaining solar Dst, 
the stakeholders involved, the formation of geomagnetic storms, and our overall objectives. We worked closely to ensure that our presentation was clear, 
well-organized and met the assessment criteria to maximize our chances of getting the best marks. Collaboration was key, and we made sure to align our parts for a cohesive delivery.

